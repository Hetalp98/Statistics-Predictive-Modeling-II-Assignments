# Statistics-Predictive-Modeling-II-Assignments

This repository contains my work for Statistical and Predictive Modeling II with Professor Fatma Tetikoglu.  
Across six assignments and one final project, I applied a variety of machine learning and statistical techniques to solve real-world business problems using Python, Jupyter Notebook, and PowerPoint presentations.

Each project includes:
- A Jupyter Notebook (HTML) with full code and analysis
- A PowerPoint presentation summarizing findings
- Supporting datasets or dataset sources

**Assignment 1 â€“ Multiple Regression**
Dataset: `diamonds.csv` (53,940 observations, 10 variables)  
Objective: Create a multivariate regression model to forecast diamond prices using carat, cut, color, clarity, and dimensions.  
Techniques:
- One-Hot Encoding for categorical variables
- Model evaluation using Adjusted RÂ², MAE, RMSE
- Coefficient interpretation
- Deliverables:** Regression equation, metric analysis, recommendations

---

**Assignment 2 â€“ Logistic Regression**
Dataset:`WheatData.csv` (210 observations, 8 variables)  
Objective: Build a logistic regression model with ROC/AUC and Learning Curves to classify wheat type (Kama, Rosa, Canadian).  
Techniques:
- Learning curve analysis
- Confusion Matrix & Classification Report insights
- ROC/AUC evaluation  
Deliverables: Model metrics, insights, recommendations

---

**Assignment 3 â€“ Discriminant Analysis**
Dataset: WheatData.csv` (210 observations, 8 variables
Objective: Apply discriminant analysis techniques for classification and compare model performance.  
Techniques:
- Linear & Quadratic Discriminant Analysis
- Visualization of decision boundaries
- Evaluation metrics and recommendations

---

**Assignment 4 â€“ Regularization**
Dataset: `EnergyUseHeating.csv` (768 observations, 9 variables)  
Objective: Use LASSO, Ridge, and Elastic Net regression to forecast heating load after outlier removal with Tukeyâ€™s method.  
Techniques:
- Outlier removal (Tukey method)
- Model comparison using Adjusted RÂ², MAE, RMSE
- Feature selection impact via LASSO  
Deliverables: Model outputs, insights, recommended next steps

---

**Assignment 5 â€“ Decision Trees & Random Forest**
Dataset: `WheatData.csv` (210 observations, 8 variables)  
Objective: Build optimized Decision Tree and Random Forest models, compare performance, and identify key features.  
Techniques:
- Model tuning
- Classification Report insights (Precision, Recall, F1)
- Feature importance ranking  
Deliverables: Model analysis, box plots, recommendations

---

**Final Project â€“ Wireless Customer Churn Prediction**
Dataset: `wireless_churn.csv` (3,333 observations, 11 variables)  
Objective: Predict customer churn using Logistic Regression, NaÃ¯ve Bayes, and a Voting Ensemble model.  
Techniques:
- EDA with pandas-profiling
- Outlier removal using Isolation Forest
- Learning Curves with recall-weighted scoring
- Optimized model tuning & ROC/AUC comparison
- Ensemble method with bagging/boosting  
Deliverables: Model performance comparison, recommendations for deployment

---

 **Technologies Used**
- Python (pandas, numpy, scikit-learn, matplotlib, seaborn, pandas-profiling)
- Jupyter Notebook (HTML exports)
- PowerPoint for reporting
- Machine Learning Models:
  - Regression (Linear, LASSO, Ridge, Elastic Net)
  - Logistic Regression
  - Decision Tree, Random Forest
  - Discriminant Analysis
  - NaÃ¯ve Bayes
  - Ensemble Learning

---

## ðŸ“Œ Notes
- Datasets are either included if public or linked to their source.
- All notebooks are exported to HTML for easy viewing without Jupyter.
- Each assignment folder contains its **README.md** for specific details.

---

##  Author
**Hetal Parmar**  
Postgraduate Certificate in Data Analytics | Postgraduate Certificate in GIS  
